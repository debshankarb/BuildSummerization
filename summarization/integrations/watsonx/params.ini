[Default]
    DECODING_METHOD = greedy
    MAX_NEW_TOKENS = 500
    MIN_NEW_TOKENS = 1
    STREAM = False
    INPUT_TEXT = False
    INPUT_TOKENS = False
    TEMPERATURE = 0.8
    REPETITION_PENALTY = 1.0
    MODEL = meta-llama/llama-2-70b-chat

[MajorIncident]
    DECODING_METHOD = sample
    MAX_NEW_TOKENS = 525
    MIN_NEW_TOKENS = 1
    STREAM = False
    INPUT_TEXT = False
    INPUT_TOKENS = False
    TOP_K = 25
    TEMPERATURE = 0.4
    REPETITION_PENALTY = 1.2
    RANDOM_SEED = 3254856538
    MODEL = meta-llama/llama-2-70b-chat

[Telemetry]
    DECODING_METHOD = sample
    MAX_NEW_TOKENS = 1536
    MIN_NEW_TOKENS = 1
    STREAM = False
    TOP_K = 5
    TEMPERATURE = 1
    REPETITION_PENALTY = 1
    RANDOM_SEED = 2289750979
    MODEL = meta-llama/llama-2-70b-chat




           
    