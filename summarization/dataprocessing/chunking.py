# Assuming text is a long string that you wish to split
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dataprocessing.summary import Summary
from transformers import BertTokenizer
from model_classes import ExceptionMessageEnum,CustomException
from fastapi import HTTPException
from config import log_entry_exit
import configparser

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

config = configparser.ConfigParser()
config.read("constants.ini")
INPUT_LIMIT = int(config.get('SUMMARIZATION','INPUT_LIMIT'))
CHUNKING_THRESHOLD = int(config.get('SUMMARIZATION','CHUNKING_THRESHOLD'))

class Chunk:

    @log_entry_exit
    def token_count(self, text):
        tokens = tokenizer.tokenize(text)
        token_count = len(tokens)
        return token_count

    @log_entry_exit
    def create_chunks(self, text):
        custom_text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=7000,
            chunk_overlap=150,
            length_function=len
        )

        texts = custom_text_splitter.create_documents([text])
        return texts

    @log_entry_exit
    def summarize_chunks(self,chunks,summary_type):

        if summary_type == 'build_summary_structured':
            summary_type = 'build_summary_chunk'
        else:
            summary_type = 'long_summary'

        long_summary = Summary.generate_summary(chunks, summary_type)
        
        return long_summary

    @log_entry_exit
    def summarize(self,texts,summary_type):
        
        token_count = self.token_count(texts)
        if token_count < INPUT_LIMIT:
            if token_count > CHUNKING_THRESHOLD:
                texts = self.create_chunks(texts)

                chunks = [text_doc.page_content for text_doc in texts]

                summaries = self.summarize_chunks(chunks,summary_type)

                aggregate_summary = ''.join(summaries)
            else:
                aggregate_summary = texts
        else:
            #  token_count*1.25 is used here because BAM Token Counts is always 1.25 times greater than tokens generated by BERT,GPT2...         
            raise CustomException(
                error_code=422,
                message=ExceptionMessageEnum.INPUT_LIMIT_REACHED.value.format(token = int(token_count*1.25)))

        return Summary.generate_summary([aggregate_summary], summary_type)


